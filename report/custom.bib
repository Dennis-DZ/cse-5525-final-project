@misc{huynh2025largelanguagemodelscode,
    title={Large Language Models for Code Generation: A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications}, 
    author={Nam Huynh and Beiyu Lin},
    year={2025},
    eprint={2503.01245},
    archivePrefix={arXiv},
    primaryClass={cs.SE},
    url={https://arxiv.org/abs/2503.01245}, 
}

@inproceedings{10.1145/3708035.3736091,
    author = {Liu, Fengchen and Jung, Gary},
    title = {Exploring Few-Shot Learning: Fine-Tuning vs. In-Context Learning and Parameter-Efficient Adaptations},
    year = {2025},
    isbn = {9798400713989},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3708035.3736091},
    doi = {10.1145/3708035.3736091},
    abstract = {Few-shot learning with pre-trained language models has become a cornerstone of modern natural language processing, enabling rapid adaptation to new tasks with minimal labeled data. This project explores and compares three approaches to few-shot learning: fine-tuning, in-context learning, and parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation). Using three datasets—GLUE (classification), HotpotQA (question answering), and Multi-News (summarization)—we evaluate the performance of each approach across various k-shot settings (k ∈ {1, 4, 8, 16, 32, 64, 128, 256, 512, 1024}). Key findings reveal that fine-tuning achieves strong performance but is resource-intensive; in-context learning, while efficient in terms of parameters, underperforms across all tasks; and LoRA offers a compelling compromise, achieving competitive performance with theoretically lower resource demands, making it ideal for low-resource scenarios. This study provides practical insights into the trade-offs between these methods and offers guidelines for selecting the most suitable approach based on task requirements and available resources.},
    booktitle = {Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration},
    articleno = {31},
    numpages = {4},
    keywords = {few-shot learning, fine-tuning, in-context learning, LoRA, natural language processing},
    location = {
    },
    series = {PEARC '25}
}

@misc{cherian2024llmphycomplexphysicalreasoning,
    title={LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models}, 
    author={Anoop Cherian and Radu Corcodel and Siddarth Jain and Diego Romeres},
    year={2024},
    eprint={2411.08027},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2411.08027}, 
}

@misc{shafiq2025powersmallllmsgeometry,
    title={The Power of Small LLMs in Geometry Generation for Physical Simulations}, 
    author={Ossama Shafiq and Bahman Ghiassi and Alessio Alexiadis},
    year={2025},
    eprint={2503.18178},
    archivePrefix={arXiv},
    primaryClass={cs.CE},
    url={https://arxiv.org/abs/2503.18178}, 
}

@article{ALEXIADIS2024101721,
    title = {From text to tech: Shaping the future of physics-based simulations with AI-driven generative models},
    journal = {Results in Engineering},
    volume = {21},
    pages = {101721},
    year = {2024},
    issn = {2590-1230},
    doi = {https://doi.org/10.1016/j.rineng.2023.101721},
    url = {https://www.sciencedirect.com/science/article/pii/S2590123023008484},
    author = {Alessio Alexiadis and Bahman Ghiassi},
    keywords = {Multiphysics software, Physics-informed machine learning, Computational fluid dynamics software, Coupling large language models with Physics-based simulations, Generative AI in engineering},
    abstract = {This micro-article introduces a method for integrating Large Language Models with geometry/mesh generation software and multiphysics solvers, aimed at streamlining physics-based simulations. Users provide simulation descriptions in natural language, which the language model processes for geometry/mesh generation and physical model definition. Initial results demonstrate the feasibility of this approach, suggesting a future where non-experts can conduct advanced multiphysics simulations by simply describing their needs in natural language, while the code autonomously handles complex tasks like geometry building, meshing, and setting boundary conditions.}
}

@misc{alrashedy2025generatingcadcodevisionlanguage,
    title={Generating CAD Code with Vision-Language Models for 3D Designs}, 
    author={Kamel Alrashedy and Pradyumna Tambwekar and Zulfiqar Zaidi and Megan Langwasser and Wei Xu and Matthew Gombolay},
    year={2025},
    eprint={2410.05340},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2410.05340}, 
}

@misc{Tim2018zss,
	author = {{Tim Henderson}},
	year = {2018},
	month = {mar 12},
	publisher = {GitHub},
	title = {zss},
	url = {https://www.github.com/timtadh/zhang-shasha},
}

@article{doi:10.1137/0218082,
    author   = {Zhang, Kaizhong and Shasha, Dennis},
    title    = {Simple Fast Algorithms for the Editing Distance between Trees and Related Problems},
    journal  = {SIAM Journal on Computing},
    volume   = {18},
    number   = {6},
    pages    = {1245-1262},
    year     = {1989},
    doi      = {10.1137/0218082},
    url      = {https://doi.org/10.1137/0218082},
    eprint   = {https://doi.org/10.1137/0218082},
}

@misc{Schopplich2025toon,
	author = {Schopplich, Johann and Horn, Beech},
	year = {2025},
	month = {dec 15},
	title = {toon-format/spec},
	url = {https://github.com/toon-format/spec},
	howpublished = {https://github.com/toon-format/spec},
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{hui2024qwen25codertechnicalreport,
      title={Qwen2.5-Coder Technical Report}, 
      author={Binyuan Hui and Jian Yang and Zeyu Cui and Jiaxi Yang and Dayiheng Liu and Lei Zhang and Tianyu Liu and Jiajun Zhang and Bowen Yu and Keming Lu and Kai Dang and Yang Fan and Yichang Zhang and An Yang and Rui Men and Fei Huang and Bo Zheng and Yibo Miao and Shanghaoran Quan and Yunlong Feng and Xingzhang Ren and Xuancheng Ren and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12186}, 
}
