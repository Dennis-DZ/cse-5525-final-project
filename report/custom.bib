@misc{huynh2025largelanguagemodelscode,
    title={Large Language Models for Code Generation: A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications}, 
    author={Nam Huynh and Beiyu Lin},
    year={2025},
    eprint={2503.01245},
    archivePrefix={arXiv},
    primaryClass={cs.SE},
    url={https://arxiv.org/abs/2503.01245}, 
}

@inproceedings{10.1145/3708035.3736091,
    author = {Liu, Fengchen and Jung, Gary},
    title = {Exploring Few-Shot Learning: Fine-Tuning vs. In-Context Learning and Parameter-Efficient Adaptations},
    year = {2025},
    isbn = {9798400713989},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3708035.3736091},
    doi = {10.1145/3708035.3736091},
    abstract = {Few-shot learning with pre-trained language models has become a cornerstone of modern natural language processing, enabling rapid adaptation to new tasks with minimal labeled data. This project explores and compares three approaches to few-shot learning: fine-tuning, in-context learning, and parameter-efficient fine-tuning using LoRA (Low-Rank Adaptation). Using three datasets—GLUE (classification), HotpotQA (question answering), and Multi-News (summarization)—we evaluate the performance of each approach across various k-shot settings (k ∈ {1, 4, 8, 16, 32, 64, 128, 256, 512, 1024}). Key findings reveal that fine-tuning achieves strong performance but is resource-intensive; in-context learning, while efficient in terms of parameters, underperforms across all tasks; and LoRA offers a compelling compromise, achieving competitive performance with theoretically lower resource demands, making it ideal for low-resource scenarios. This study provides practical insights into the trade-offs between these methods and offers guidelines for selecting the most suitable approach based on task requirements and available resources.},
    booktitle = {Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration},
    articleno = {31},
    numpages = {4},
    keywords = {few-shot learning, fine-tuning, in-context learning, LoRA, natural language processing},
    location = {
    },
    series = {PEARC '25}
}

@misc{cherian2024llmphycomplexphysicalreasoning,
    title={LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models}, 
    author={Anoop Cherian and Radu Corcodel and Siddarth Jain and Diego Romeres},
    year={2024},
    eprint={2411.08027},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2411.08027}, 
}

@misc{shafiq2025powersmallllmsgeometry,
    title={The Power of Small LLMs in Geometry Generation for Physical Simulations}, 
    author={Ossama Shafiq and Bahman Ghiassi and Alessio Alexiadis},
    year={2025},
    eprint={2503.18178},
    archivePrefix={arXiv},
    primaryClass={cs.CE},
    url={https://arxiv.org/abs/2503.18178}, 
}

@article{ALEXIADIS2024101721,
    title = {From text to tech: Shaping the future of physics-based simulations with AI-driven generative models},
    journal = {Results in Engineering},
    volume = {21},
    pages = {101721},
    year = {2024},
    issn = {2590-1230},
    doi = {https://doi.org/10.1016/j.rineng.2023.101721},
    url = {https://www.sciencedirect.com/science/article/pii/S2590123023008484},
    author = {Alessio Alexiadis and Bahman Ghiassi},
    keywords = {Multiphysics software, Physics-informed machine learning, Computational fluid dynamics software, Coupling large language models with Physics-based simulations, Generative AI in engineering},
    abstract = {This micro-article introduces a method for integrating Large Language Models with geometry/mesh generation software and multiphysics solvers, aimed at streamlining physics-based simulations. Users provide simulation descriptions in natural language, which the language model processes for geometry/mesh generation and physical model definition. Initial results demonstrate the feasibility of this approach, suggesting a future where non-experts can conduct advanced multiphysics simulations by simply describing their needs in natural language, while the code autonomously handles complex tasks like geometry building, meshing, and setting boundary conditions.}
}

@misc{alrashedy2025generatingcadcodevisionlanguage,
    title={Generating CAD Code with Vision-Language Models for 3D Designs}, 
    author={Kamel Alrashedy and Pradyumna Tambwekar and Zulfiqar Zaidi and Megan Langwasser and Wei Xu and Matthew Gombolay},
    year={2025},
    eprint={2410.05340},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2410.05340}, 
}
